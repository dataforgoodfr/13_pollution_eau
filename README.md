# Rendre Visible la Pollution de l'Eau Potable üíß

## Contexte du Projet

Ce projet, d√©velopp√© par des b√©n√©voles de [Data For Good](https://www.dataforgood.fr/) lors de la saison 13, vise √† cr√©er une carte interactive pour [G√©n√©rations Futures](https://www.generations-futures.fr/).

L'objectif est de consolider, analyser et cartographier les donn√©es sur la qualit√© de l'eau potable en France √† partir de sources de donn√©es ouvertes.

## Structure du Projet

- `pipelines/` : Consolidation et pr√©paration des donn√©es
- `analytics/` : Analyse des donn√©es
- `webapp/` : D√©veloppement du site web interactif

## Installation

### Data Pipelines

Installer [uv](https://docs.astral.sh/uv/getting-started/installation/#installing-uv). Ce projet utilise uv pour la gestion des d√©pendances Python.

  Installation sur Windows

  ```bash
  powershell -c "irm https://astral.sh/uv/install.ps1 | iex"
  ```

  Installation sur Mac ou linux

  ```bash
  curl -LsSf https://astral.sh/uv/install.sh | sh
  ```

- Lancez la commande suivante pour installer la version de Python ad√©quate, cr√©er un environnement virtuel et installer les d√©pendances du projet.

```bash
uv sync
```

#### VSCode

A l'usage, si vous utilisez VSCode, l'environnement virtuel sera automatiquement activ√© lorsque vous ouvrirez le projet. Sinon, il suffit de l'activer manuellement avec la commande suivante :

```bash
source .venv/bin/activate
```

Ou alors, utilisez la commande `uv run ...` (au lieu de `python ...`) pour lancer un script Python. Par exemple:

```bash
uv run pipelines/run.py run build_database
```

#### Pycharm

Allez dans settings, python interpreter, add interpreter, puis selectionnez existing venv et allez chercher le path du python executable dans .venv (.venv/Scripts/Python.exe pour windows)

#### Terminal

utilisez les commandes `uv run` pour lancer un script Python depuis votre terminal

### Site web

- Installez [Node.js](https://nodejs.org/) ou [NVM](https://github.com/nvm-sh/nvm?tab=readme-ov-file#install--update-script)

  ```bash
  curl -o- https://raw.githubusercontent.com/nvm-sh/nvm/v0.40.1/install.sh | bash
  nvm install 22
  ```

- Installez les d√©pendances du site web:

  ```bash
  cd webapp
  npm install
  ```

- Lancer `npm run dev` et ouvrir le navigateur sur http://localhost:3000 pour voir la carte.

## Data Processing

### Package installation

Tout le code dans pipelines sera install√© en tant que package python automatiquement √† chaque uv_sync

### Comment construire la database

Une fois l'environnement python setup avec uv, vous pouvez lancer data_pipeline/run.py pour remplir la database

Le t√©l√©chargement des donn√©es peut se faire de plusieurs mani√®res :
1. T√©l√©chargement des donn√©es de la derni√®re ann√©e (par d√©faut)
```bash
uv run pipelines/run.py run build_database --refresh-type last
```

2. T√©l√©chargement de toutes les donn√©es

```bash
uv run pipelines/run.py run build_database --refresh-type all
```

3. T√©l√©chargement de donn√©es d'ann√©es sp√©cifiques
```bash
uv run pipelines/run.py run build_database --refresh-type custom --custom-years 2018,2024,...
```

4. Suppression des tables, puis t√©l√©chargement des donn√©es de la derni√®re ann√©e
```bash
uv run pipelines/run.py run build_database --refresh-type last --drop-tables
```

### Cr√©ation du mod√®les de donn√©es avec dbt
#### 1. Commandes a ex√©cuter
La librarie dbt est celle choisie pour une construction rapide et simple de mod√®les de donn√©es optimis√© pour l'analytics.

üö©**Remarque** : Pour lancer chaque commande individuellement, veillez √† bien vous placer dans le dossier dbt_ (`cd dbt_`) avant de lancer les commandes.

La commande `uv run dbt deps` permet de t√©l√©charger les d√©pendances du projet dbt.
Ex√©cut√©e lors de la cr√©ation de la base de donn√©es, la commande `uv run dbt build` est une commande qui permet de r√©aliser l'ensemble des actions suivantes :
* Lancer la cr√©ation des tables issues des donn√©es brutes (`uv run dbt run`)
* R√©aliser les test de qualit√© des donn√©es (`uv run dbt test`)
* Mettre sous forme de table les fichiers csv ajout√©s dans le dossiers seeds (`uv run dbt seed`)

Une autre commande `uv run dbt docs generate` permet de g√©n√©rer la documentation des mod√®les de donn√©es renseign√©e dans les fichiers `_xxx__models.yml` au format html. L'utilisation de la commande `uv run dbt docs serve` permet de lancer un serveur local pour visualiser la documentation.

Pour plus d'informations concernant la mani√®re d'organiser un projet dbt, se r√©f√©rer √† la [documentation officielle](https://docs.getdbt.com/docs/introduction) et notamment √† la section .

#### 2. Structure des donn√©es

Les mod√®les de donn√©es sont organis√©s dans le dossier `dbt_/models`. La structure suit les recommandations de la [documentation officielle](https://docs.getdbt.com/best-practices/how-we-structure/1-guide-overview). Il est conseill√© prendre le temps la lire afin de bien comprendre la structure du projet:

* **models/staging/** : Mod√®les de donn√©es avec des transformation basiques (TRIM, REPLACE, typage, ...). Cette couche est surtout utilis√©e pour faire un √©tat des donn√©es existantes, les documenter et tester la qualit√©.
* **models/intermediate/** : Mod√®les de donn√©es avec des transformation plus complexes (GROUP BY, JOIN, WHERE, ...). Cette couche est surtout utile pour faire une jointure entre les diff√©rentes tables et faire un premier filtrage des donn√©es. Celle-ci est tr√®s utile pour de l'analyse de donn√©es
* **models/analytics/** : Mod√®les de donn√©es final, qui est requ√™ter par le site web pour construire les visualisations. Cette donn√©e est propre et la sch√©matisation des donn√©es est optimis√©e pour le chargement des visualisations.

#### Documentation
La documentation du projet dbt est disponible sur le lien suivant: [documentation dbt](https://dataforgood.fr/13_pollution_eau/#!/overview)

### Comment t√©l√©charger la base de donn√©es

#### Via HTTPS

Vous pouvez simplement t√©l√©charger la base de donn√©es en cliquant sur le lien de t√©l√©chargement suivant:  https://pollution-eau-s3.s3.fr-par.scw.cloud/prod/database/data.duckdb

Vous pouvez √©galement lancer la commande suivante :
```bash
uv run pipelines/run.py run download_database
```
Elle t√©l√©chargera la base, et la placera √† l'emplacement utilis√© par tout le monde (√† savoir, `database/data.duckdb`). Un raccourci pour cette commande est accessible en un clic dans la barre des t√¢ches de VS Code (ligne tout en bas) : "Download Dabatase".

### Depuis Scaleway via [boto3](https://github.com/boto/boto3) pour stockage objet S3

Des versions de d√©veloppement et de production de la base de donn√©es sont √† disposition sur le stockage object.

Un module a √©t√© cr√©√© dans [storage_client.py](pipelines%2Futils%2Fstorage_client.py) pour faciliter la connection au S3 h√©berg√© sur Scaleway. Il faut bien configurer ses *credentials* Scaleway et son environnement. Pour cela, il faut cr√©er un fichier `.env` dans le dossier [pipelines/config](pipelines%2Fconfig), avec les secrets ci-dessous dedans pour que la connexion fonctionne :

```text
SCW_ACCESS_KEY={ACCESS_KEY}
SCW_SECRET_KEY={SECRET_KEY}
```
o√π `{ACCESS_KEY}` et `{SECRET_KEY}` sont les *credentials* obtenus via le coffre-fort vaultwarden mis en place (pour cela, il suffit de demander √† un chef de projet sur Slack).

Vous trouverez un exemple avec le fichier [.env.example](pipelines%2Fconfig%2F.env.example)

> ‚ö† **Attention:** Ne jamais faire de *commit* des *access key* et *secret key*.

Une fois les credentials obtenus et mis dans le fichier `pipelines/config/.env`, vous pouvez alors lancer la commande suivante :

```bash
uv run pipelines/run.py run download_database --use-boto3
```
Vous pouvez √©galement sp√©cifier l'option `--env {dev|prod}`.

Le notebook [test_storage_utils.ipynb](pipelines%2Fnotebooks%2Ftest_storage_utils.ipynb) montre un exemple d'utilisation de l'utils pour charger et lire des csv sur le bucket S3 du projet.

### Data analysis

Les analyses se font via jupyter notebook

```bash
uv run jupyter notebook
```

## Tests

Pour lancer les tests, il suffit de lancer la commande suivante √† la racine du projet:

```bash
uv run pytest -s
```

L'option `-s` permet d'afficher les prints dans le terminal.

## Pre Commit

Lancer la commande suivante pour s'assurer que le code satisfait bien tous les pre commit avant de cr√©er votre pull request

```bash
pre-commit run --all-files
```

## D√©ploiement du site avec Docker

Un fichier `Dockerfile` est disponible pour d√©ployer le site web avec Docker.

Pour cr√©er et ex√©cuter l'image Docker en local (√† la racine du projet) :

```bash
docker build -t pollution-eau-app .

docker run -p 8080:8080 --rm pollution-eau-app
```

Le site sera alors accessible √† l'adresse http://localhost:8080.

## How to contribute
Pour contribuer, il est recommand√© d'utiliser un fork du projet. Cela permet d'√©viter la gestion des demandes d'acc√®s au d√©p√¥t principal.

* Dans un premier temps, cliquez sur Fork pour r√©cup√©rer le projet dans votre espace GitHub.
* Cr√©ez votre branche de travail √† partir de la branche main, en respectant la nomenclature suivante :
  * feature/nom_de_la_feature pour une nouvelle fonctionnalit√©
  * hotfix/nom_du_hotfix pour une correction rapide
* Poussez votre code vers votre d√©p√¥t distant.
* Cr√©ez une pull request en sp√©cifiant :
  * Base repository : dataforgood/13_pollution_eau/main
  * Head repository : YourGithubAccount/13_pollution_eau/your_branch
* Pour faciliter la revue de la pull request :
  * Liez la pull request √† un ticket NocoDB en ajoutant le lien du ticket dans la description.
  * R√©digez une description d√©taill√©e de la pull request afin de fournir un maximum d‚Äôinformations sur les modifications apport√©es.
