# Documentation Technique

Ce document contient toutes les informations techniques n√©cessaires pour d√©velopper et contribuer au projet.

## Stack Technique

Le projet utilise ces technologies :

- **Python** (avec uv pour la gestion des d√©pendances) : pour le traitement des donn√©es (pipelines et dbt)
- **DuckDB** : pour la base de donn√©es
- **Pmtiles** : pour la diffusion des tuiles cartographiques (contenant les donn√©es)
- **Next.js** (React) : pour le site web interactif

La structure du projet est la suivante :

- `pipelines/` : Consolidation et pr√©paration des donn√©es
- `dbt_/` : Mod√®les de donn√©es et transformations
- `analytics/` : Analyses et notebooks (n'est plus utilis√© depuis la fin de la saison c√†d avril 2025)
- `webapp/` : Site web Next.js interactif
- `database/` : Base de donn√©es DuckDB et fichiers de cache

## Installation

### 1. Data Pipelines (Python) et database (DuckDB)

Installer [uv](https://docs.astral.sh/uv/getting-started/installation/#installing-uv). Ce projet utilise uv pour la gestion des d√©pendances Python.

Lancez la commande suivante pour installer la version de Python ad√©quate, cr√©er un environnement virtuel et installer les d√©pendances du projet :

```bash
uv sync
```

L'ensemble des donn√©es du projet est stock√© dans une database DuckDB: `database/data.duckdb`. Vous pouvez le t√©l√©charger en utilisant la commande suivante :

```bash
uv run pipelines/run.py run download_database
```

### 2. Site Web (Next.js)

Installez [Node.js](https://nodejs.org/) ou [NVM](https://github.com/nvm-sh/nvm?tab=readme-ov-file#install--update-script) :

Installez les d√©pendances du site web :

```bash
cd webapp
npm install
```

Lancez le serveur de d√©veloppement :

```bash
npm run dev
```

Le site sera accessible sur http://localhost:3000

## Commandes Principales

### Pipeline complet (comme en CI/CD)

Pour reconstruire compl√®tement la base de donn√©es et g√©n√©rer les fichiers pour le site web :

```bash
# 1. T√©l√©charger toutes les donn√©es sources
uv run pipelines/run.py run build_database --refresh-type all
# Ou, travailler avec la base de donn√©es existante :
# uv run pipelines/run.py run download_database

# 2. Construire les mod√®les de donn√©es avec dbt
cd dbt_
uv run dbt build
cd ..

# 3. G√©n√©rer les fichiers PMTiles pour la cartographie
uv run pipelines/run.py run generate_pmtiles

# 4. D√©placer les fichiers PMTiles pour le d√©veloppement local
mv database/cache/commune_data.pmtiles webapp/public/pmtiles/commune_data.pmtiles
mv database/cache/udi_data.pmtiles webapp/public/pmtiles/udi_data.pmtiles

# 5. Lancer le site web en local
cd webapp && npm run dev
```

### Commandes utiles

Le script `pipelines/run.py` expose plusieurs commandes utiles :

#### Gestion de la base de donn√©es

```bash
# T√©l√©charger la base de donn√©es depuis le cloud
uv run pipelines/run.py run download_database

# T√©l√©charger la base de donn√©es depuis Scaleway via boto3 (n√©cessite credentials)
uv run pipelines/run.py run download_database --use-boto3

# Sp√©cifier l'environnement (dev ou prod)
uv run pipelines/run.py run download_database --env prod
```

#### Construction de la base de donn√©es

```bash
# T√©l√©chargement des donn√©es de la derni√®re ann√©e (par d√©faut)
uv run pipelines/run.py run build_database --refresh-type last

# T√©l√©chargement de toutes les donn√©es
uv run pipelines/run.py run build_database --refresh-type all

# T√©l√©chargement de donn√©es d'ann√©es sp√©cifiques
uv run pipelines/run.py run build_database --refresh-type custom --custom-years 2018,2024

# Suppression des tables, puis t√©l√©chargement
uv run pipelines/run.py run build_database --refresh-type last --drop-tables

# Rafra√Æchir une table sp√©cifique
uv run pipelines/run.py run build_database --refresh-table edc
```

#### G√©n√©ration et gestion des PMTiles

```bash
# G√©n√©rer les fichiers PMTiles (n√©cessite uv pip install .[pmtiles])
uv run pipelines/run.py run generate_pmtiles
```

#### Liste compl√®te des commandes disponibles

Pour voir toutes les commandes disponibles :

```bash
uv run pipelines/run.py --help
```

Pour voir l'aide d'une commande sp√©cifique :

```bash
uv run pipelines/run.py run build_database --help
```

## Mod√®les de donn√©es (dbt)

### Commandes dbt

üö© **Remarque** : Pour lancer chaque commande individuellement, veillez √† bien vous placer dans le dossier dbt\_ (`cd dbt_`) avant de lancer les commandes.

```bash
# T√©l√©charger les d√©pendances dbt
uv run dbt deps

# Construire tous les mod√®les et ex√©cuter les tests
uv run dbt build

# Cr√©er uniquement les tables
uv run dbt run

# Ex√©cuter uniquement les tests de qualit√©
uv run dbt test

# Charger les fichiers CSV du dossier seeds
uv run dbt seed

# G√©n√©rer la documentation
uv run dbt docs generate

# Visualiser la documentation (serveur local)
uv run dbt docs serve
```

### Documentation des mod√®les de donn√©es

dbt g√©n√®re automatiquement une documentation interactive de tous les mod√®les de donn√©es, incluant :

- La description de chaque table et colonne
- Les relations entre les tables
- Les tests de qualit√© appliqu√©s
- Le lineage (graphe de d√©pendances) des donn√©es

Pour consulter cette documentation en local :

```bash
# 1. Se placer dans le dossier dbt_
cd dbt_

# 2. G√©n√©rer la documentation
uv run dbt docs generate

# 3. Lancer le serveur local
uv run dbt docs serve
```

La documentation sera accessible sur http://localhost:8080

### Structure des donn√©es

Les mod√®les de donn√©es sont organis√©s dans le dossier `dbt_/models`. La structure suit les recommandations de la [documentation officielle dbt](https://docs.getdbt.com/best-practices/how-we-structure/1-guide-overview) :

- **models/staging/** : Transformations basiques (TRIM, REPLACE, typage, ...). Cette couche documente et teste la qualit√© des donn√©es brutes.
- **models/intermediate/** : Transformations complexes (GROUP BY, JOIN, WHERE, ...). Utile pour joindre les tables et filtrer les donn√©es pour l'analyse.
- **models/website/** : Mod√®les finaux requ√™t√©s par le site web. Donn√©es propres et sch√©ma optimis√© pour les visualisations.

## Configuration Scaleway (S3)

Pour acc√©der au stockage S3 h√©berg√© sur Scaleway, cr√©ez un fichier `.env` dans le dossier `pipelines/config` :

```text
SCW_ACCESS_KEY={ACCESS_KEY}
SCW_SECRET_KEY={SECRET_KEY}
```

Les credentials sont disponibles via le coffre-fort vaultwarden (demander √† un chef de projet sur Slack).

Vous trouverez un exemple avec le fichier [.env.example](pipelines/config/.env.example)

## Analyse de donn√©es

Les analyses se font via Jupyter Notebook :

```bash
uv run jupyter notebook
```

## Tests

Pour lancer les tests √† la racine du projet :

```bash
uv run pytest -s
```

L'option `-s` permet d'afficher les prints dans le terminal.

## Pre-commit

Assurez-vous que le code satisfait tous les pre-commit avant de cr√©er votre pull request :

```bash
uv run pre-commit run --all-files
```

## Architecture technique du frontend

Cette section explique comment le site web Next.js re√ßoit et affiche les donn√©es de pollution de l'eau.

### Sources de donn√©es

Le frontend consomme les donn√©es de trois sources principales :

#### 1. Fichiers PMTiles (donn√©es cartographiques et r√©sultats par zone)

Les fichiers PMTiles sont g√©n√©r√©s par le pipeline Python (commande `generate_pmtiles`) et stock√©s dans `webapp/public/pmtiles/` :

- `udi_data.pmtiles` : Donn√©es des r√©seaux d'eau (UDI)
- `commune_data.pmtiles` : Donn√©es des communes

**Contenu des PMTiles :**

- G√©om√©tries vectorielles de chaque zone (trac√©s des UDI/communes)
- Donn√©es de pollution pour chaque zone (r√©sultats d'analyses, concentrations, dates, etc.)

**Utilisation dans le code :**

- **Affichage des popups** (`webapp/components/PollutionMapMarker.tsx`) : Lorsqu'un utilisateur clique sur une zone, le composant `PollutionMapMarker` r√©cup√®re les donn√©es de la zone cliqu√©e via la variable `selectedZoneData` et les utilise pour afficher les r√©sultats de pollution dans une popup.
- **Coloration de la carte** (`webapp/lib/colorMapping.ts`) : La fonction `generateColorExpression()` g√©n√®re des expressions MapLibre GL qui d√©finissent la couleur de chaque zone en fonction de ses r√©sultats de pollution.

#### 2. Configuration des polluants (fichier statique)

Le fichier `webapp/lib/polluants.ts` contient la configuration statique de toutes les cat√©gories de polluants :

- Liste compl√®te des polluants suivis (PFAS, pesticides, nitrates, CVM, perchlorate, etc.)
- Labels d'affichage en fran√ßais
- Palettes de couleurs pour chaque seuil de pollution
- etc.

Cette configuration est utilis√©e partout dans le frontend pour l'affichage des r√©sultats.

#### 3. Base de donn√©es DuckDB (donn√©es agr√©g√©es)

Le fichier `database/data.duckdb` (r√©duit en production via `trim_database_for_website`) est utilis√© pour :

**Donn√©es du site web Next.js** (`webapp/app/lib/data.ts`) :

- `fetchPollutionStats()` : Statistiques globales affich√©es sur la page d'accueil (nombre d'UDI pollu√©es, etc.)
- `fetchParameterValues()` : Valeurs de r√©f√©rence des param√®tres (limites r√©glementaires, labels) depuis la table `int__valeurs_de_reference`

**Routes API** (`webapp/app/api/`) :
Ces routes exposent des donn√©es en JSON pour des usages externes au site.

### Flux de donn√©es

```
Pipeline Python (generate_pmtiles)
    ‚Üì
PMTiles (udi_data.pmtiles, commune_data.pmtiles)
    ‚Üì
MapLibre GL (carte interactive)
    ‚Üì
PollutionMapMarker (affichage des r√©sultats)
```

```
Base DuckDB (data.duckdb)
    ‚Üì
Fonctions fetchPollutionStats / fetchParameterValues
    ‚Üì
Page d'accueil + Composants frontend
```
